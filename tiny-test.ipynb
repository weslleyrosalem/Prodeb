{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U einops langchain pymilvus sentence-transformers openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do servidor de inferência e do modelo\n",
    "INFERENCE_SERVER_URL = \"http://vllm.llm-hosting.svc.cluster.local:8000/v1\"\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MAX_TOKENS = 1024\n",
    "TOP_P = 0.95\n",
    "TEMPERATURE = 0.01\n",
    "PRESENCE_PENALTY = 1.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template do prompt\n",
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "Você é um assistente prestativo, respeitoso e honesto chamado HatBot respondendo perguntas.\n",
    "Você receberá uma pergunta que precisa responder com base no seu conhecimento.\n",
    "Sempre responda da forma mais útil possível, enquanto se mantém seguro. Suas respostas não devem incluir conteúdo prejudicial, antiético, racista, sexista, tóxico, perigoso ou ilegal. Por favor, assegure-se de que suas respostas são socialmente imparciais e positivas.\n",
    "\n",
    "Se uma pergunta não fizer sentido ou não for factualmente coerente, explique o motivo em vez de responder algo incorreto. Se você não souber a resposta para uma pergunta, por favor, não compartilhe informações falsas.\n",
    "<</SYS>>\n",
    "\n",
    "Pergunta: {question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Configuração do modelo LLM\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para consulta direta ao modelo\n",
    "class DirectLLMQA:\n",
    "    def __init__(self, llm, prompt_template):\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def run(self, question):\n",
    "        prompt = self.prompt_template.format(question=question)\n",
    "        response = self.llm.invoke({\"query\": prompt})\n",
    "        return response\n",
    "\n",
    "# Instancia o objeto de consulta direta ao LLM\n",
    "direct_qa = DirectLLMQA(llm=llm, prompt_template=QA_CHAIN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "def main():\n",
    "    question = \"Quais são os benefícios do uso de IA em ambientes empresariais?\"\n",
    "    response = direct_qa.run(question)\n",
    "    print(\"Resposta do modelo:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
