{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9ca21a-269f-4a6c-ad2a-3669dcbaed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/opt/app-root/src/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/40b98394640e630d5276807046089b233113aa87/modeling_hf_nomic_bert.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0 openai==1.13.3\n",
    "\n",
    "import os\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "# Parâmetros de conexão e modelo\n",
    "INFERENCE_SERVER_URL = \"http://vllm.vllm.svc.cluster.local:8000/v1\"\n",
    "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "MAX_TOKENS = 1024\n",
    "TOP_P = 0.95\n",
    "TEMPERATURE = 0.01\n",
    "PRESENCE_PENALTY = 1.03\n",
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = \"root\"\n",
    "MILVUS_PASSWORD = \"Milvus\"\n",
    "MILVUS_COLLECTION = \"catalogo_ba_gov\"\n",
    "\n",
    "model_kwargs = {'trust_remote_code': True}\n",
    "\n",
    "# Cria a função de embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "# Conecta ao Milvus (vectorstore)\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"host\": MILVUS_HOST,\n",
    "        \"port\": MILVUS_PORT,\n",
    "        \"user\": MILVUS_USERNAME,\n",
    "        \"password\": MILVUS_PASSWORD\n",
    "    },\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    ")\n",
    "\n",
    "# Template do prompt com orientação para o LLM\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Você é um assistente prestativo que deve responder apenas com base nas informações fornecidas no contexto.\n",
    "Se o contexto não fornecer informações relevantes para responder à pergunta, responda: \"Não tenho informações suficientes para responder a essa pergunta.\" ou peça esclarecimentos.\n",
    "<</SYS>>\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Configuração do LLM via VLLMOpenAI\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4, \"score_threshold\": 0.7} \n",
    "    ),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a779f67f-1c38-4314-ba0a-66364f1218c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para responder, você deve fornecer os passos necessários para a Emissão de 2ª Via de IPTU. \n",
      "\n",
      "Por favor, responda com base nas informações fornecidas no contexto.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "---\n",
      "\n",
      "A pergunta está em inglês e não está claro o que você precisa da resposta. Por favor, forneça mais detalhes ou contexto para que eu possa ajudar melhor. \n",
      "\n",
      "Se você tiver mais informações, posso tentar ajudar a responder a sua pergunta. No entanto, se você não for capaz de fornecer as informações necessárias, estou aqui para ajudar a responder a sua pergunta."
     ]
    }
   ],
   "source": [
    "# Exemplo de consulta\n",
    "question = \"Qual o passo a passo para renovar meu IPVA\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77c6f0d-a7d6-4770-a8dd-71e9ef98566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown/IPTU.md\n"
     ]
    }
   ],
   "source": [
    "# Função para remover duplicatas (com base na fonte)\n",
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47947954-9546-4b0d-9a2b-dfebcb17ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
